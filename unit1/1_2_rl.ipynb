{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is reinforcement learning?  \n",
    "* An agent learns by interacting with its environment where it receives feedback (positive/negative) depending on the actions it takes  \n",
    "\n",
    "Or, more formally: \"Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent receives a State from the Environment.  \n",
    "based on this State, the Agent takes an Action.  \n",
    "(Agent is in a new Environment).  \n",
    "Agent receives a new State from the new Environment.  \n",
    "the new Environment gives the Agent some Reward.  \n",
    "\n",
    "The Agents goal is to maximise the cumaltive reward.  \n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg\" width=\"60%\">  \n",
    "\n",
    "This demonstrates Markov Decision Process. Basically, this implies the Agent only needs to know the current State to decide which Action to take (not the entire history of states).  \n",
    "\n",
    "### State vs Observation  \n",
    "* State: seeing a complete description of state for the entire environment (a chess board)  \n",
    "* Observation: seeing a partial description of state for the partially observed environment (flappy bird)  \n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/obs_space_recap.jpg\" width=\"60%\">  \n",
    "\n",
    "### Action Space  \n",
    "The action space is the set of all possible actions in an environment. The action space can be discrete and finate (left,right,up,down), or continuous and infinite.  \n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/action_space.jpg\" width=\"60%\">\n",
    "\n",
    "### Rewards and the discounting  \n",
    "Rewards are the only feedback the Agent receives. It tells the Agent whether the Action taken was good or bad.  \n",
    "\n",
    "The cumalative reward can be written as:  \n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_1.jpg\">\n",
    "But in reality, the rewards can't be added like this. The rewards at the start of the game, for example, are more likely to happen than those later in the game. We balance this by discounting the rewards.  \n",
    "\n",
    "1. Define a discount rate gamma between 0 and 1 (typically 0.95-0.99).   \n",
    "  a. Higher number (0.99) = smaller discount = agent cares more about long-term reward.  \n",
    "  b. Lower number (0.95) = higher discount = agent cares more about short-term reward.   \n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_3.jpg\" width=\"50%\">  \n",
    "\n",
    "2. \"Then, each reward will be discounted by gamma to the exponent of the time step\". In this example, as the time steps increase, and the cat gets closer, and so the further away reward is less and less likely to happen.  \n",
    "\n",
    "Discounted expected cumaltive reward:  \n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_4.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
