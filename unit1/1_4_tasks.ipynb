{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of tasks  \n",
    "Episodic and continuing.  \n",
    "\n",
    "### Episodic  \n",
    "* A starting point and an ending point (a terminal state)  \n",
    "* Mario Brothers - from the starting point to the flag at the end  \n",
    "* Episode of states, actions, rewards, and new states  \n",
    "\n",
    "### Continuing  \n",
    "* No terminal state  \n",
    "* The agent must \"learn how to choose the best actions and simultaneously interact with the environment\"  \n",
    "* The agent keeps going until we stop it  \n",
    "* Example: trading (stock market)  \n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/tasks.jpg\" width=\"60%\">  \n",
    "\n",
    "# Exploration / Exploitation trade-off  \n",
    "* Exploration: \"exploring the environment by trying random actions in order to find more information about the environment\"  \n",
    "* Exploitation: \"exploiting known information to maximize the reward\"  \n",
    "\n",
    "Example: in a situation where there are many nearby, infinite rewards, the agent operating in an exploitation way will stay in this area. However, if it were to explore a bit more, it might discover an even larger reward further away.  \n",
    "\n",
    "The trade-off is between how much we want to explore the environment verses exploiting what we already know.  \n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/expexpltradeoff.jpg\" width=\"60%\">  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
